---
title: "Bike Share Case Study"
author: "Andy Guo"
date: "`r Sys.Date()`"
output: "html_notebook"
---

For my Google Data Analytics capstone project, I completed a case study for a fictional bike-share company in which I analyzed 12 months of data to understand the difference between casual riders and annual members. From these insights, the marketing team can develop a strategy to convert casual riders into annual members. To accomplish my task, I applied the data analysis process including ask, prepare, process, analyze, share, and act.

#### Ask the Right Questions

Cyclistic is a bike-share company in Chicago with a fleet of 5,824 geotracked bicycles and a network of 692 stations. They offer 3 pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders and customers who purchase annual memberships are referred to as Cyclistic members. The company determined that annual members are more profitable than casual riders and is aiming to create a marketing campaign to maximize the number of annual members by converting casual riders.

**Key stakeholder**: Lily Moreno, the director of marketing responsible for developing campaigns to promote bike-share program.

**Business task:** How do annual members and casual riders use Cyclistic bikes differently?

#### Prepare Data

Since Cyclistic is a fictional company, we want to use proxy data from a similar bike-share company which can be downloaded from : <https://divvy-tripdata.s3.amazonaws.com/index.html>

After downloading 12 ZIP files, each containing a CSV file with data for each month between June 2021 and May 2022, install and load required packages, then import and bind all 12 data sets into a single data frame. `read_csv` will automatically extract ZIP files.

```{r load_packages}
#Install and load required packages.
library(tidyverse) 
library(lubridate) 
library(hms)
library(janitor)
```

```{r import_and_bind}
#Import and bind 12 datasets into single data frame. 
bikedata1 <- list.files(pattern = "*.csv") %>% 
  lapply(read_csv) %>% 
  bind_rows()
```

Data Information

-   There are 13 columns and 5860776 rows.

-   Data types include 2 datetime, 4 double, and 7 character.

-   There are missing values in start_station_name, start_station_id, end_station_name, end_station_id

```{r preview_data}
#Preview data for immediate insights
str(bikedata1)
head(bikedata1)
glimpse(bikedata1)
colnames(bikedata1)
```

#### Process and Clean Data for Analysis

We want to separate the date and add individual columns for the month, day, year, and day of the week. This will allow us to aggregate data and perform calculations easier. We also want to find the length of each ride using the `difftime()` function, then convert it to a format that is easy to read (HH:MM:SS).

```{r add_date_columns}
bikedata1$date <- as_date(bikedata1$started_at)

#Add column for month
bikedata1$month <- format(as_date(bikedata1$date), "%m")

#Add column for day
bikedata1$day <- format(as_date(bikedata1$date), "%d")

#Add column for year
bikedata1$year <- format(as_date(bikedata1$date), "%Y")

#Add Cclumn for day of the week
bikedata1$day_of_week <- format(as_date(bikedata1$started_at), "%A")
```

```{r add_ride_length}
#Add column for ride length
bikedata1$ride_length <- difftime(bikedata1$ended_at, bikedata1$started_at) %>% as_hms()
```

We want to remove columns start_lat, start_lng, end_lat, and end_lng as the data was dropped starting in 2020. We also need to decide how to handle missing values which were primarily in start_station_name, start_station_id, end_station_name, and end_station_id. We could try to find the missing stations using the latitude/longitude but the numbers are not accurate to definitively pinpoint the correct station. That leaves us with either removing the variable (column) or observation (row). I decided to remove the rows with missing values to minimize inaccurate data because the missing values may have influenced the start/end times. For example, a bike could have been docked incorrectly and the time continued to run.

```{r remove_dirty_data}
#Remove lat/lng columns
bikedata1 <- bikedata1 %>% select(-start_lat, -start_lng, -end_lat, -end_lng)

#Remove rows with missing values
bikedata1 <- na.omit(bikedata1)
```

We want to verify the data and look for any inconsistent or inaccurate data. We confirmed that there are no duplicates, each ride is either member or casual, and the data is within the correct date range. However, there were 3 ride types (electric_bike, classic_bike, and docked_bike) which was not expected. After reviewing the types of bikes available on the [Divvy Bike website](https://divvybikes.com/how-it-works/meet-the-bikes), there there is no mention of a docked bike. I decided to remove rows with docked bikes to minimize the risk of inaccurate date.

```{r verify_data}
#Check for duplicates
bikedata1 %>% get_dupes(ride_id)

#Check ride type
bikedata1 %>% count(rideable_type)

#Check member type
bikedata1 %>% count(member_casual)

#Check for data outside date range
bikedata1 %>% filter(started_at <= as_date("2021-05-31") | started_at >= as_date("2022-06-01"))
```

```{r remove_docked_bike}
bikedata1 <- bikedata1 %>% filter(!rideable_type == "docked_bike")
```

Analyze Data

```{r summary_ride_length}
summary(as.numeric(bikedata1$ride_length)) / 60
```

```{r}
aggregate(list(ride_length = bikedata1$ride_length), list(member_type = bikedata1$member_casual), mean)

as.numeric(mean(bikedata1$ride_length), units = "mins")
```

Share Data